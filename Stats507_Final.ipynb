{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0010abb",
   "metadata": {},
   "source": [
    "## data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb52ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# For visualization\n",
    "import torchvision.transforms.functional as TF\n",
    "class EmoSet(Dataset):\n",
    "    ATTRIBUTES_MULTI_CLASS = [\n",
    "        'scene', 'facial_expression', 'human_action', 'brightness', 'colorfulness',\n",
    "    ]\n",
    "    ATTRIBUTES_MULTI_LABEL = [\n",
    "        'object'\n",
    "    ]\n",
    "    NUM_CLASSES = {\n",
    "        'brightness': 11,\n",
    "        'colorfulness': 11,\n",
    "        'scene': 254,\n",
    "        'object': 409,\n",
    "        'facial_expression': 6,\n",
    "        'human_action': 264,\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_root,\n",
    "                 num_emotion_classes,\n",
    "                 phase,\n",
    "                 ):\n",
    "        assert num_emotion_classes in (8, 2), \"num_emotion_classes must be either 8 or 2\"\n",
    "        assert phase in ('train', 'val', 'test'), \"phase must be 'train', 'val', or 'test'\"\n",
    "        self.transforms_dict = self.get_data_transforms()\n",
    "\n",
    "        self.info = self.get_info(data_root, num_emotion_classes)\n",
    "\n",
    "        if phase == 'train':\n",
    "            self.transform = self.transforms_dict['train']\n",
    "        elif phase == 'val':\n",
    "            self.transform = self.transforms_dict['val']\n",
    "        elif phase == 'test':\n",
    "            self.transform = self.transforms_dict['test']\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        split_file = os.path.join(data_root, f'{phase}.json')\n",
    "        with open(split_file, 'r') as f:\n",
    "            data_store = json.load(f)\n",
    "\n",
    "        self.data_store = [\n",
    "            [\n",
    "                self.info['emotion']['label2idx'][item[0]],\n",
    "                int(item[1].split('.')[0].split('_')[-1]),\n",
    "                os.path.join(data_root, item[1]),\n",
    "                os.path.join(data_root, item[2])\n",
    "            ]\n",
    "            for item in data_store\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_data_transforms(cls):\n",
    "        transforms_dict = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "        }\n",
    "        return transforms_dict\n",
    "\n",
    "    def get_info(self, data_root, num_emotion_classes):\n",
    "        assert num_emotion_classes in (8, 2), \"num_emotion_classes must be either 8 or 2\"\n",
    "        info_path = os.path.join(data_root, 'info.json')\n",
    "        with open(info_path, 'r') as f:\n",
    "            info = json.load(f)\n",
    "\n",
    "        if num_emotion_classes == 8:\n",
    "            # Ensure 'emotion' key exists with label2idx and idx2label\n",
    "            if 'emotion' not in info:\n",
    "                info['emotion'] = {\n",
    "                    'label2idx': info.get('label2idx', {}),\n",
    "                    'idx2label': info.get('idx2label', {})\n",
    "                }\n",
    "        elif num_emotion_classes == 2:\n",
    "            # Map emotions to 'positive' and 'negative'\n",
    "            emotion_info = {\n",
    "                'label2idx': {\n",
    "                    'amusement': 0,\n",
    "                    'awe': 0,\n",
    "                    'contentment': 0,\n",
    "                    'excitement': 0,\n",
    "                    'anger': 1,\n",
    "                    'disgust': 1,\n",
    "                    'fear': 1,\n",
    "                    'sadness': 1,\n",
    "                },\n",
    "                'idx2label': {\n",
    "                    '0': 'positive',\n",
    "                    '1': 'negative',\n",
    "                }\n",
    "            }\n",
    "            info['emotion'] = emotion_info\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return info\n",
    "\n",
    "    def load_image_by_path(self, path):\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def load_annotation_by_path(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        return json_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emotion_label_idx, image_id, image_path, annotation_path = self.data_store[idx]\n",
    "        image = self.load_image_by_path(image_path)\n",
    "        annotation_data = self.load_annotation_by_path(annotation_path)\n",
    "        data = {'image_id': image_id, 'image': image, 'emotion_label_idx': emotion_label_idx}\n",
    "\n",
    "        # Process multi-class attributes\n",
    "        for attribute in self.ATTRIBUTES_MULTI_CLASS:\n",
    "            attribute_label_idx = -1  # Default to -1 if not present\n",
    "            if attribute in annotation_data:\n",
    "                label = str(annotation_data[attribute])  # Ensure label is string for dict keys\n",
    "                \n",
    "                attribute_label_idx = self.info['label2idx'].get(label, -1)\n",
    "            data[f'{attribute}_label_idx'] = attribute_label_idx\n",
    "\n",
    "        # Process multi-label attributes\n",
    "        for attribute in self.ATTRIBUTES_MULTI_LABEL:\n",
    "            assert attribute == 'object', \"Currently only 'object' attribute is supported as multi-label\"\n",
    "            num_classes = self.NUM_CLASSES[attribute]\n",
    "            attribute_label_idx = torch.zeros(num_classes, dtype=torch.float)\n",
    "            if attribute in annotation_data:\n",
    "                for label in annotation_data[attribute]:\n",
    "                    idx_label = self.info['label2idx'].get(label, None)\n",
    "                    if idx_label is not None and 0 <= idx_label < num_classes:\n",
    "                        attribute_label_idx[idx_label] = 1.0\n",
    "            data[f'{attribute}_label_idx'] = attribute_label_idx\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_store)\n",
    "\n",
    "\n",
    "def get_transforms(phase='train'):\n",
    "    if phase == 'train':\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "\n",
    "def create_dataloaders(data_root, batch_size=32, num_workers=4, num_emotion_classes=8):\n",
    "    \"\"\"\n",
    "    Creates train, validation, and test dataloaders.\n",
    "\n",
    "    Args:\n",
    "        data_root (str): Root directory of the dataset.\n",
    "        batch_size (int): Batch size.\n",
    "        num_workers (int): Number of subprocesses for data loading.\n",
    "        num_emotion_classes (int): Number of emotion classes (2 or 8).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='train')\n",
    "    val_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='val')\n",
    "    test_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='test')\n",
    "\n",
    "    # Handle class imbalance with weighted sampler\n",
    "    # print(train_dataset.data_store)\n",
    "    # print(len(train_dataset.data_store))\n",
    "    # print((train_dataset.data_store[0]))\n",
    "    labels = [sample[0] for sample in train_dataset.data_store]\n",
    "    if num_emotion_classes == 2:\n",
    "        label_indices = labels\n",
    "    else:\n",
    "        label_indices = labels\n",
    "\n",
    "    class_sample_count = np.array([labels.count(t) for t in range(num_emotion_classes)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[label] for label in label_indices])\n",
    "    samples_weight = torch.from_numpy(samples_weight).double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def create_sub_dataset(dataset, fraction=0.1):\n",
    "    size = len(dataset)\n",
    "    subset_size = int(size * fraction)\n",
    "    indices = list(range(size))\n",
    "    random.shuffle(indices)\n",
    "    subset_indices = indices[:subset_size]\n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "\n",
    "def create_dataloaders(data_root, batch_size=32, num_workers=4, num_emotion_classes=8, fraction=0.1):\n",
    "    \"\"\"\n",
    "    Creates train, validation, and test dataloaders and uses only a fraction of each dataset.\n",
    "\n",
    "    Args:\n",
    "        data_root (str): Root directory of the dataset.\n",
    "        batch_size (int): Batch size.\n",
    "        num_workers (int): Number of subprocesses for data loading.\n",
    "        num_emotion_classes (int): Number of emotion classes (2 or 8).\n",
    "        fraction (float): Fraction of dataset to use, e.g., 0.1 for 1/10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='train')\n",
    "    val_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='val')\n",
    "    test_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='test')\n",
    "\n",
    "    # 10% data\n",
    "    train_dataset = create_sub_dataset(train_dataset, fraction)\n",
    "    val_dataset = create_sub_dataset(val_dataset, fraction)\n",
    "    test_dataset = create_sub_dataset(test_dataset, fraction)\n",
    "\n",
    "    # Handle class imbalance with weighted sampler\n",
    "    labels = [sample['emotion_label_idx'] for sample in train_dataset]\n",
    "    class_sample_count = np.array([labels.count(t) for t in range(num_emotion_classes)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[label] for label in labels])\n",
    "    samples_weight = torch.from_numpy(samples_weight).double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c075f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './'\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(data_root=data_root, batch_size=32, num_workers=4, num_emotion_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474aca84",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "class ViTSentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=8, pretrained=True):\n",
    "        super(ViTSentimentClassifier, self).__init__()\n",
    "        self.vit = vit_b_16(pretrained=pretrained)\n",
    "        self.vit.heads = nn.Identity()  \n",
    "        self.classifier = nn.Linear(self.vit.hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.vit(x)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "class CLIPSentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=8, pretrained=True):\n",
    "        super(CLIPSentimentClassifier, self).__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"clip-vit-base-patch16\")\n",
    "        self.classifier = nn.Linear(self.clip_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.clip_model.get_image_features(images)\n",
    "        out = self.classifier(outputs)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ad7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "class ViTSentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=8, pretrained=True):\n",
    "        super(ViTSentimentClassifier, self).__init__()\n",
    "        self.vit = vit_b_16(pretrained=pretrained)\n",
    "        self.vit.heads = nn.Identity()  \n",
    "        self.classifier = nn.Linear(self.vit.hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.vit(x)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "class CLIPSentimentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=8, pretrained=True):\n",
    "        super(CLIPSentimentClassifier, self).__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"./clip/\")\n",
    "        print(self.clip_model.config)\n",
    "        self.classifier = nn.Linear(self.clip_model.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, images):\n",
    "        outputs = self.clip_model.get_image_features(images)\n",
    "        out = self.classifier(outputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36e76e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20, scheduler=None):\n",
    "    \"\"\"\n",
    "    Trains the given model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        train_loader (DataLoader): Training data loader.\n",
    "        val_loader (DataLoader): Validation data loader.\n",
    "        criterion: Loss function.\n",
    "        optimizer: Optimizer.\n",
    "        device: Device to train on ('cuda:2' or 'cpu').\n",
    "        num_epochs (int): Number of epochs.\n",
    "        scheduler: Learning rate scheduler.\n",
    "\n",
    "    Returns:\n",
    "        model: Trained model.\n",
    "        history: Training history containing loss and accuracy.\n",
    "    \"\"\"\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['emotion_label_idx'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_preds += torch.sum(preds == labels).item()\n",
    "            total_preds += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct_preds = 0\n",
    "        val_total_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['emotion_label_idx'].to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct_preds += torch.sum(preds == labels).item()\n",
    "                val_total_preds += labels.size(0)\n",
    "\n",
    "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = val_correct_preds / val_total_preds\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_epoch_loss:.4f} | Val Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    return model, history\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model\n",
    "num_emotion_classes = 8  # or 2 for binary classification\n",
    "vit_model = ViTSentimentClassifier(num_classes=num_emotion_classes, pretrained=True)\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(vit_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "trained_vit_model, vit_history = train_model(\n",
    "    model=vit_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "# Initialize the CLIP-based model\n",
    "clip_model = CLIPSentimentClassifier(num_classes=num_emotion_classes, pretrained=True)\n",
    "clip_model = clip_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion_clip = nn.CrossEntropyLoss()\n",
    "optimizer_clip = torch.optim.AdamW(clip_model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler_clip = torch.optim.lr_scheduler.StepLR(optimizer_clip, step_size=5, gamma=0.1)\n",
    "\n",
    "# Train the CLIP model\n",
    "trained_clip_model, clip_history = train_model(\n",
    "    model=clip_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion_clip,\n",
    "    optimizer=optimizer_clip,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    scheduler=scheduler_clip\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3733e5b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ce443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, num_classes=8):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        test_loader (DataLoader): Test data loader.\n",
    "        device: Device to perform evaluation on.\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        metrics: Dictionary containing accuracy, precision, recall, and F1-score.\n",
    "        conf_matrix: Confusion matrix.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['emotion_label_idx'].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "    return metrics, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_metrics, vit_conf_matrix = evaluate_model(trained_vit_model, test_loader, device, num_classes=num_emotion_classes)\n",
    "print(\"ViT Model Evaluation Metrics:\")\n",
    "print(vit_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_metrics, clip_conf_matrix = evaluate_model(trained_clip_model, test_loader, device, num_classes=num_emotion_classes)\n",
    "print(\"CLIP Model Evaluation Metrics:\")\n",
    "print(clip_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, classes, title='Confusion Matrix'):\n",
    "    \"\"\"\n",
    "    Plots the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        conf_matrix (np.array): Confusion matrix.\n",
    "        classes (list): List of class names.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Load idx2label from info.json\n",
    "def load_idx2label(data_root, num_emotion_classes):\n",
    "    info_path = os.path.join(data_root, 'info.json')\n",
    "    with open(info_path, 'r') as f:\n",
    "        info = json.load(f)\n",
    "    if num_emotion_classes == 8:\n",
    "        idx2label = {int(k): v for k, v in info['idx2label'].items()}\n",
    "    elif num_emotion_classes == 2:\n",
    "        idx2label = {0: 'positive', 1: 'negative'}\n",
    "    return idx2label\n",
    "\n",
    "idx2label = load_idx2label(data_root, num_emotion_classes=8)\n",
    "\n",
    "# Plot for ViT\n",
    "plot_confusion_matrix(vit_conf_matrix, classes=[idx2label[i] for i in range(num_emotion_classes)],\n",
    "                      title='ViT Model Confusion Matrix')\n",
    "\n",
    "# Plot for CLIP\n",
    "plot_confusion_matrix(clip_conf_matrix, classes=[idx2label[i] for i in range(num_emotion_classes)],\n",
    "                      title='CLIP Model Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f7f19",
   "metadata": {},
   "source": [
    "## Visualization with Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency_map_vit(model, image, label, device):\n",
    "    \"\"\"\n",
    "    Generates a saliency map for the given image using ViT.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained ViT model.\n",
    "        image (Tensor): Input image tensor.\n",
    "        label (int): True label.\n",
    "        device: Device.\n",
    "\n",
    "    Returns:\n",
    "        saliency: Saliency map.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    image.requires_grad_()\n",
    "\n",
    "    output = model(image)\n",
    "    loss = nn.CrossEntropyLoss()(output, torch.tensor([label]).to(device))\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    saliency, _ = torch.max(image.grad.data.abs(), dim=1)\n",
    "    saliency = saliency.squeeze().cpu().numpy()\n",
    "\n",
    "    return saliency\n",
    "\n",
    "\n",
    "sample_batch = next(iter(test_loader))\n",
    "sample_image = sample_batch['image'][0]\n",
    "sample_label = sample_batch['emotion_label_idx'][0]\n",
    "saliency = generate_saliency_map_vit(trained_vit_model, sample_image, sample_label, device)\n",
    "print(idx2label,sample_label)\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(sample_image.permute(1, 2, 0).cpu().numpy())\n",
    "plt.imshow(saliency, cmap='hot', alpha=0.5)\n",
    "plt.title(f\"Saliency Map - True Label: {idx2label[sample_label.item()]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8006b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency_map_clip(model, image, label, device):\n",
    "    \"\"\"\n",
    "    Generates a saliency map for the given image using CLIP.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained CLIP model.\n",
    "        image (Tensor): Input image tensor.\n",
    "        label (int): True label.\n",
    "        device: Device.\n",
    "\n",
    "    Returns:\n",
    "        saliency: Saliency map.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    image.requires_grad_()\n",
    "\n",
    "    output = model(image)\n",
    "    loss = nn.CrossEntropyLoss()(output, torch.tensor([label]).to(device))\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    saliency, _ = torch.max(image.grad.data.abs(), dim=1)\n",
    "    saliency = saliency.squeeze().cpu().numpy()\n",
    "\n",
    "    return saliency\n",
    "\n",
    "saliency_clip = generate_saliency_map_clip(trained_clip_model, sample_image, sample_label, device)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(sample_image.permute(1, 2, 0).cpu().numpy())\n",
    "plt.imshow(saliency_clip, cmap='hot', alpha=0.5)\n",
    "plt.title(f\"Saliency Map (CLIP) - True Label: {idx2label[sample_label.item()]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
